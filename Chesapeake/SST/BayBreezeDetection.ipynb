{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Chesapeake'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-56fa84cf920a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelper_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalc_wind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsout_seriesReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mChesapeake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaybreezedict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetectBayBreeze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcartopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mccrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcartopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcfeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Chesapeake'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "from os import path\n",
    "import xarray as xr\n",
    "from matplotlib.colors import Normalize\n",
    "import sys\n",
    "from collections import Counter\n",
    "sys.path.append('/Users/hawbecke/Code/Python/publications/')\n",
    "from mmctools.helper_functions import calc_uv\n",
    "from mmctools.helper_functions import calc_wind\n",
    "from mmctools.wrf.utils import Tower, tsout_seriesReader\n",
    "from Chesapeake.baybreezedict import DetectBayBreeze\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.shapereader as shpreader\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to save the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir  = '/Users/hawbecke/Research/Chesapeake/Papers/SST_Paper/img/'\n",
    "save_figs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in ASOS data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asos_path = '/Users/hawbecke/Research/Chesapeake/Data/Obs/ASOS/ASOS_2019to2020.nc'\n",
    "#asos_path = '/Users/hawbecke/Research/Chesapeake/Data/Obs/ASOS/5min/ASOS_2007to2019.nc'\n",
    "\n",
    "if path.exists(asos_path):\n",
    "    print('loading in full dataset!')\n",
    "    asos_ds = xr.open_dataset(asos_path)\n",
    "else:\n",
    "    print('Please run AWOS_Station_Locations notebook to get data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in AWOS data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awos_path = '/Users/hawbecke/Research/Chesapeake/Data/Obs/AWOS/AWOS_2019to2020.nc'\n",
    "#awos_path = '/Users/hawbecke/Research/Chesapeake/Data/Obs/AWOS/2007_to_2020/AWOS_2007to2020.nc'\n",
    "\n",
    "if path.exists(awos_path):\n",
    "    print('loading in full dataset!')\n",
    "    awos_ds = xr.open_dataset(awos_path)\n",
    "else:\n",
    "    print('Please run AWOS_Station_Locations notebook to get data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in APG data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apg_path = '/Users/hawbecke/Research/Chesapeake/Data/Obs/APG/APG_data_2019.nc'\n",
    "if path.exists(apg_path):\n",
    "    print('loading in full dataset!')\n",
    "    apg_ds = xr.open_dataset(apg_path)\n",
    "else:\n",
    "    print('Please run AWOS_Station_Locations notebook to get data')\n",
    "apg_stn_list = list(apg_ds.station.data)\n",
    "for ss,apg_stn in enumerate(apg_stn_list):\n",
    "    if apg_stn == 'PAA':\n",
    "        apg_stn_list[ss] = 'APG'\n",
    "apg_ds = apg_ds.assign_coords({'station': apg_stn_list})\n",
    "non_apg_list = apg_stn_list.copy()\n",
    "non_apg_list.remove('APG')\n",
    "apg_ds = apg_ds.drop_sel(station=non_apg_list)\n",
    "apg_ds = apg_ds.drop(['rh','gust','radt','rain','alt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine AWOS and APG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = awos_ds.sel(station='APG').combine_first(apg_ds)\n",
    "temp = temp.squeeze()\n",
    "temp = temp.expand_dims('station')\n",
    "\n",
    "lat_da = xr.DataArray([float(apg_ds.lat)], dims=('station'))\n",
    "lon_da = xr.DataArray([float(apg_ds.lon)], dims=('station'))\n",
    "\n",
    "temp = temp.assign_coords({'lat':lat_da})\n",
    "temp = temp.assign_coords({'lon':lon_da})\n",
    "\n",
    "awos_ds = awos_ds.drop_sel(station=['APG'])\n",
    "\n",
    "awos_ds = xr.merge([awos_ds,temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot combined AWOS and ASOS near-shore stations and IAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awos_near_shore_stations  = []\n",
    "asos_near_shore_stations  = []\n",
    "for stn in awos_ds.get_index('station'):\n",
    "    if awos_ds.sel(station=stn).region.data == 'coastal':\n",
    "        awos_near_shore_stations.append(stn)\n",
    "                                         \n",
    "for stn in asos_ds.get_index('station'):\n",
    "    if asos_ds.sel(station=stn).region.data == 'coastal':\n",
    "        asos_near_shore_stations.append(stn)\n",
    "\n",
    "\n",
    "both_near_shore_stations = []\n",
    "for stn in awos_near_shore_stations:\n",
    "    if stn in asos_near_shore_stations:\n",
    "        both_near_shore_stations.append(stn)\n",
    "\n",
    "for stn in both_near_shore_stations:\n",
    "    if stn in awos_near_shore_stations:\n",
    "        awos_near_shore_stations.remove(stn)\n",
    "    if stn in asos_near_shore_stations:\n",
    "        asos_near_shore_stations.remove(stn)\n",
    "\n",
    "station_dict = {}\n",
    "for stn in awos_near_shore_stations:\n",
    "    station_dict[stn] = {'lat':float(awos_ds.sel(station=stn).lat.data),\n",
    "                         'lon':float(awos_ds.sel(station=stn).lon.data),\n",
    "                         'min':float(awos_ds.sel(station=stn).onshore_min.data),\n",
    "                         'max':float(awos_ds.sel(station=stn).onshore_max.data),\n",
    "                           #'c':'#e3e022'}\n",
    "                           'c':'goldenrod'}\n",
    "for stn in asos_near_shore_stations:\n",
    "    station_dict[stn] = {'lat':float(asos_ds.sel(station=stn).lat.data),\n",
    "                         'lon':float(asos_ds.sel(station=stn).lon.data),\n",
    "                         'min':float(asos_ds.sel(station=stn).onshore_min.data),\n",
    "                         'max':float(asos_ds.sel(station=stn).onshore_max.data),\n",
    "                           'c':'b'}\n",
    "for stn in both_near_shore_stations:\n",
    "    station_dict[stn] = {'lat':float(awos_ds.sel(station=stn).lat.data),\n",
    "                         'lon':float(awos_ds.sel(station=stn).lon.data),\n",
    "                         'min':float(awos_ds.sel(station=stn).onshore_min.data),\n",
    "                         'max':float(awos_ds.sel(station=stn).onshore_max.data),\n",
    "                           'c':'coral'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in WRF-calculated min/max:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onshore_min_max_path = '/Users/hawbecke/Research/Chesapeake/Data/WRF/20190716to20190801/onshore_min_max.nc'\n",
    "if path.exists(onshore_min_max_path):\n",
    "    print('loading in onshore min/max dataset!')\n",
    "    wrf_onshore_min_max_ds = xr.open_dataset(onshore_min_max_path)\n",
    "    wrf_onshore_min = wrf_onshore_min_max_ds.onshore_min\n",
    "    wrf_onshore_max = wrf_onshore_min_max_ds.onshore_max\n",
    "else:\n",
    "    print('Run BayBreezeDetectionModel.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wrf_minmax = True\n",
    "\n",
    "lon = wrf_onshore_min_max_ds.XLONG\n",
    "lat = wrf_onshore_min_max_ds.XLAT\n",
    "for ss,stn in enumerate(station_dict):\n",
    "    #if stn == 'APG':\n",
    "    #    stn_ds = apg_ds.sel(station=stn)\n",
    "    if stn in awos_ds.station:\n",
    "        stn_ds = awos_ds.sel(station=stn)\n",
    "    elif stn in asos_ds.station:\n",
    "        stn_ds = asos_ds.sel(station=stn)\n",
    "    stn_lat = stn_ds.lat.data\n",
    "    stn_lon = stn_ds.lon.data\n",
    "    dist_from_stn = ((lon-stn_lon)**2 + (lat-stn_lat)**2)**0.5\n",
    "    stn_j,stn_i = np.where(dist_from_stn == np.nanmin(dist_from_stn))\n",
    "    stn_j = int(stn_j)\n",
    "    stn_i = int(stn_i)\n",
    "    if stn == 'NHK': print('{},{}'.format(stn_i,stn_j))\n",
    "    print('Station: {}'.format(stn))\n",
    "    print('\\tOnshore min - Old: {0:4.3f}\\tNew: {1:4.3f}'.format(stn_ds.onshore_min.data,wrf_onshore_min.data[stn_j,stn_i]))\n",
    "    print('\\tOnshore max - Old: {0:4.3f}\\tNew: {1:4.3f}'.format(stn_ds.onshore_max.data,wrf_onshore_max.data[stn_j,stn_i]))\n",
    "    if use_wrf_minmax:\n",
    "        station_dict[stn]['min'] = wrf_onshore_min.data[stn_j,stn_i]\n",
    "        station_dict[stn]['max'] = wrf_onshore_max.data[stn_j,stn_i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buoy_ds = xr.open_dataset('/Users/hawbecke/Research/Chesapeake/Data/Obs/Buoy/combined_buoy_data_res_2019.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = '10m'\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 7.5), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "\n",
    "land = cfeature.NaturalEarthFeature('physical', 'land', resolution,\n",
    "                                    edgecolor='face',\n",
    "                                    facecolor=\"#ddf0d1\")\n",
    "\n",
    "water = cfeature.NaturalEarthFeature('physical', 'ocean', resolution,\n",
    "                                    edgecolor='face',\n",
    "                                    facecolor=\"#ebf9ff\")\n",
    "\n",
    "shapename = 'admin_1_states_provinces_lakes_shp'\n",
    "states_shp = shpreader.natural_earth(resolution=resolution,\n",
    "                                     category='cultural', name=shapename)\n",
    "\n",
    "def colorize_state(geometry):\n",
    "    facecolor = (0.9375, 0.9375, 0.859375)\n",
    "    return {'facecolor': 'None', 'edgecolor': 'black', 'alpha':0.25}\n",
    "\n",
    "for axi in range(len(ax)):\n",
    "    ax[axi].add_feature(land)\n",
    "    ax[axi].add_feature(water)\n",
    "    ax[axi].add_geometries(shpreader.Reader(states_shp).geometries(),\n",
    "                         ccrs.PlateCarree(),\n",
    "                         styler=colorize_state)\n",
    "    ax[axi].coastlines('10m',alpha=0.25)\n",
    "\n",
    "center_lat = 38.7\n",
    "center_lon = -76.4\n",
    "map_1 = [center_lon-1.2, center_lon+1.0, center_lat-0.9, center_lat+1.0]\n",
    "center_lat = 38.5\n",
    "map_2 = [center_lon-1.5, center_lon+2.0, center_lat-2.25, center_lat+1.30]\n",
    "\n",
    "ax[0].set_extent(map_1, ccrs.PlateCarree())\n",
    "ax[1].set_extent(map_2, ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "gl = {0:ax[0].gridlines(draw_labels=True,\n",
    "                        xlocs=np.arange(-78.25,-60,0.75),\n",
    "                        ylocs=np.arange(37,41,0.5),\n",
    "                       ),\n",
    "      1:ax[1].gridlines(draw_labels=True,\n",
    "                        xlocs=np.arange(-78.25,-60,1.0),\n",
    "                        ylocs=np.arange(33,41,1.0),\n",
    "                       )\n",
    "     }\n",
    "gl[0].ylabels_right = False\n",
    "gl[1].ylabels_right = True\n",
    "gl[1].ylabels_left = False\n",
    "\n",
    "for axi in range(len(ax)):\n",
    "    gl[axi].xlabel_style = {'size': 16, 'color': 'k'}\n",
    "    gl[axi].xformatter = LONGITUDE_FORMATTER\n",
    "    gl[axi].yformatter = LATITUDE_FORMATTER\n",
    "    gl[axi].xlabels_top = False\n",
    "    gl[axi].ylabel_style = {'size': 16, 'color': 'k'}\n",
    "    \n",
    "box_x = [map_1[0],map_1[0],map_1[1],map_1[1],map_1[0]]\n",
    "box_y = [map_1[2],map_1[3],map_1[3],map_1[2],map_1[2]]  \n",
    "ax[1].fill(box_x,box_y,zorder=2,facecolor='None',lw=2.0,edgecolor='#737373',ls='--')\n",
    "\n",
    "\n",
    "# Station Labels:\n",
    "for stn in station_dict:\n",
    "    x, y = station_dict[stn]['lon'], station_dict[stn]['lat']\n",
    "    ax[0].scatter(x,y,zorder=3,facecolor=station_dict[stn]['c'],marker='o',s=80,edgecolor='k')\n",
    "    ax[0].text(x-0.03,y,stn,size=18,ha='right',va='center',zorder=3)\n",
    "    pts  = np.arange(station_dict[stn]['min'],station_dict[stn]['max'],5.0)\n",
    "    npts = pts.size\n",
    "    fill_x = [x]\n",
    "    fill_y = [y]\n",
    "    for dd,wdir in enumerate(pts):\n",
    "        d = 270.0 - wdir    # Convert met degrees to polar\n",
    "        plt_dist = -0.30 # Met degrees are FROM dir... need negative distance!\n",
    "        fill_x.append(x+plt_dist*np.cos(np.radians(d)))\n",
    "        fill_y.append(y+plt_dist*np.sin(np.radians(d)))\n",
    "\n",
    "    ax[0].fill(fill_x, fill_y,alpha=0.35,lw=None,color=station_dict[stn]['c'],zorder=2)\n",
    "    x, y = station_dict[stn]['lon'], station_dict[stn]['lat']\n",
    "    ax[1].scatter(x,y,zorder=3,facecolor=station_dict[stn]['c'],alpha=0.5,marker='o',s=50,lw=None,edgecolor=None)\n",
    "    \n",
    "x, y = float(awos_ds.sel(station='IAD').lon.data), float(awos_ds.sel(station='IAD').lat.data)\n",
    "ax[0].scatter(x,y,zorder=3,c='k')\n",
    "ax[0].text(x,y+0.01,'IAD',size=18,ha='center',va='bottom',zorder=3)\n",
    "\n",
    "\n",
    "# Buoy Labels:\n",
    "buoys_to_plot = list(buoy_ds.station.data)\n",
    "bad_buoys = ['PMC','GRF','SRP','CAM','SLM','WDC'] # On land in WRF or Obs missing data\n",
    "\n",
    "buoy_c = 'royalblue'\n",
    "for bad_buoy in bad_buoys:\n",
    "    buoys_to_plot.remove(bad_buoy)\n",
    "for stn in buoys_to_plot:\n",
    "    buoy_lon = float(buoy_ds.sel(station=stn).lon.data)\n",
    "    buoy_lat = float(buoy_ds.sel(station=stn).lat.data)\n",
    "    if stn == 'JTN':\n",
    "        buoy_lat, buoy_lon = 37.21137, -76.78677\n",
    "    x, y = buoy_lon, buoy_lat\n",
    "    ax[1].scatter(x,y,zorder=3,c=buoy_c,marker='^',s=80)\n",
    "    xa,ya = 0.0,0.0\n",
    "    if stn == 'ANN':\n",
    "        ha,va = 'center','bottom'\n",
    "        xa,ya = 0.0,0.01\n",
    "    elif stn == 'THP':\n",
    "        ha,va = 'center','top'\n",
    "        xa,ya = 0.0,-0.04\n",
    "    elif stn == 'GOO':\n",
    "        ha,va = 'right','center'\n",
    "        xa,ya = -0.03,0.0\n",
    "    elif stn == 'GRF':\n",
    "        ha,va = 'left','center'\n",
    "        xa,ya = 0.03,0.0\n",
    "    elif stn == 'JTN':\n",
    "        ha,va = 'center','bottom'\n",
    "        xa,ya = 0.0,0.0\n",
    "    elif stn == 'LWT':\n",
    "        ha,va = 'right','center'\n",
    "        xa,ya = -0.03,0.0\n",
    "    elif stn == 'YKS':\n",
    "        ha,va = 'right','center'\n",
    "        xa,ya = -0.05,0.0\n",
    "    elif stn == 'YKP':\n",
    "        ha,va = 'left','center'\n",
    "        xa,ya = 0.03,0.0\n",
    "    elif stn == 'TBL':\n",
    "        ha,va = 'right','center'\n",
    "        xa,ya = -0.03,0.0\n",
    "    elif stn == 'FLN':\n",
    "        ha,va = 'left','bottom'\n",
    "        xa,ya = 0.03,-0.03\n",
    "    elif stn == 'FLG':\n",
    "        ha,va = 'left','top'\n",
    "        xa,ya = 0.03,-0.03\n",
    "    else:\n",
    "        ha,va = 'center','bottom'\n",
    "        xa,ya = 0.0,0.03\n",
    "    ax[1].text(x+xa,y+ya,stn,size=18,ha=ha,va=va,zorder=3)\n",
    "    x, y = buoy_lon, buoy_lat\n",
    "    ax[0].scatter(x,y,zorder=3,facecolor=buoy_c,alpha=0.35,marker='^',s=50,edgecolor=None)\n",
    "x, y = float(awos_ds.sel(station='IAD').lon.data), float(awos_ds.sel(station='IAD').lat.data)\n",
    "ax[1].scatter(x,y,zorder=3,facecolor='k',alpha=0.5,marker='o',s=50,lw=None,edgecolor=None)\n",
    "\n",
    "\n",
    "ax[0].set_title('AWOS and ASOS Station Locations',size=22)\n",
    "ax[1].set_title('Buoy Locations',size=22)\n",
    "\n",
    "props = dict(facecolor='w', alpha=1.0)\n",
    "\n",
    "# place a text box in upper left in axes coords\n",
    "map1_width  = map_1[1] - map_1[0]\n",
    "map1_height = map_1[3] - map_1[2]\n",
    "\n",
    "map2_width  = map_2[1] - map_2[0]\n",
    "map2_height = map_2[3] - map_2[2]\n",
    "\n",
    "txt_loc1 = map_1[0]+0.025*map1_width, map_1[3]-0.025*map1_height\n",
    "txt_loc2 = map_2[0]+0.025*map2_width, map_2[3]-0.025*map2_height\n",
    "ax[0].text(txt_loc1[0],txt_loc1[1], 'a.)', bbox=props, ha='left',va='top',fontsize=20)\n",
    "ax[1].text(txt_loc2[0],txt_loc2[1], 'b.)', bbox=props, ha='left',va='top',fontsize=20)\n",
    "plt.subplots_adjust(wspace=0.001,hspace=0.001)\n",
    "figname = '{}StationLocation'.format(save_dir)\n",
    "if save_figs:\n",
    "    plt.savefig('{}.png'.format(figname), bbox_inches = 'tight',pad_inches = 0.1)\n",
    "    plt.savefig('{}.pdf'.format(figname), bbox_inches = 'tight',pad_inches = 0.1)\n",
    "    print(figname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir      = '/Users/hawbecke/Research/Chesapeake/Data/Obs/'\n",
    "\n",
    "obs_type = 'ASOS'\n",
    "\n",
    "detection_methods = ['StaufferThompson2015', 'Stauffer2015', 'Sikora2010']\n",
    "\n",
    "\n",
    "# Start time:     0900 Local (1300 UTC) \n",
    "start_time = pd.to_timedelta(13,unit='h')\n",
    "# End time:       1600 Local (2000 UTC)\n",
    "end_time   = pd.to_timedelta(20,unit='h')\n",
    "\n",
    "year_range = np.arange(2019,2020,1)\n",
    "nyears = year_range.size\n",
    "\n",
    "# Dates (month-day) to analyze:\n",
    "date_start = '01-01'\n",
    "date_end   = '12-31'\n",
    "\n",
    "light_winds = 3.08667 # m/s\n",
    "#light_winds = 2.57222 # m/s\n",
    "\n",
    "sample_rate = '60min'\n",
    "\n",
    "min_points = 3\n",
    "\n",
    "show_plot = True\n",
    "\n",
    "if obs_type == 'ASOS':\n",
    "    ds_stn = asos_ds\n",
    "elif obs_type == 'AWOS':\n",
    "    ds_stn = awos_ds\n",
    "near_shore_stations  = []\n",
    "inland_station_names = []\n",
    "for stn in ds_stn.get_index('station'):\n",
    "    if ds_stn.sel(station=stn).region.data == 'inland':\n",
    "        inland_station_names.append(stn)\n",
    "    if ds_stn.sel(station=stn).region.data == 'coastal':\n",
    "        near_shore_stations.append(stn)\n",
    "\n",
    "if obs_type == 'ASOS':\n",
    "    inland_station_names = ['IAD','DCA','MRB','HGR']\n",
    "elif obs_type == 'AWOS':\n",
    "    inland_station_names = ['IAD','CJR','GAI','FDK']\n",
    "\n",
    "\n",
    "print(near_shore_stations)\n",
    "print(inland_station_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stations = sorted(np.unique(['ADW', 'APG', 'BWI', 'CBE', 'CGE', 'CGS', 'CJR', 'ESN', 'FDK',\n",
    "       'GAI', 'HGR', 'IAD', 'MTN', 'NAK', 'NHK', 'NUI', 'OXB', 'SBY',\n",
    "       'W29', 'BWI', 'DCA', 'GED', 'HGR', 'IAD', 'ILG', 'MIV', 'MRB', 'NAK',\n",
    "       'NHK', 'NUI', 'OXB', 'PHL', 'SBY', 'WAL']))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "\n",
    "land = cfeature.NaturalEarthFeature('physical', 'land', resolution,\n",
    "                                    edgecolor='face',\n",
    "                                    facecolor=\"#ddf0d1\")\n",
    "\n",
    "water = cfeature.NaturalEarthFeature('physical', 'ocean', resolution,\n",
    "                                    edgecolor='face',\n",
    "                                    facecolor=\"#ebf9ff\")\n",
    "\n",
    "shapename = 'admin_1_states_provinces_lakes_shp'\n",
    "states_shp = shpreader.natural_earth(resolution=resolution,\n",
    "                                     category='cultural', name=shapename)\n",
    "\n",
    "def colorize_state(geometry):\n",
    "    facecolor = (0.9375, 0.9375, 0.859375)\n",
    "    return {'facecolor': 'None', 'edgecolor': 'black', 'alpha':0.25}\n",
    "\n",
    "ax.add_feature(land)\n",
    "ax.add_feature(water)\n",
    "ax.add_geometries(shpreader.Reader(states_shp).geometries(),\n",
    "                     ccrs.PlateCarree(),\n",
    "                     styler=colorize_state)\n",
    "ax.coastlines('10m',alpha=0.25)\n",
    "\n",
    "center_lat = 38.690760\n",
    "center_lon = -76.401987\n",
    "map_1 = [center_lon-1.7, center_lon+1.6, center_lat-0.8, center_lat+1.3]\n",
    "\n",
    "ax.set_extent(map_1, ccrs.PlateCarree())\n",
    "\n",
    "parallels = np.arange(38.0,40.1,0.5)\n",
    "meridians = np.arange(-80.5,-74.9,0.75)\n",
    "\n",
    "gl = ax.gridlines(draw_labels=True,\n",
    "                     xlocs=meridians,\n",
    "                     ylocs=parallels,\n",
    "                    )\n",
    "gl.xlabel_style = {'size': 16, 'color': 'k'}\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabels_top = False\n",
    "gl.ylabel_style = {'size': 16, 'color': 'k'}\n",
    "\n",
    "\n",
    "#m.drawparallels(parallels,labels=[True,False,True,False],color='#93994d',size=14)\n",
    "#m.drawmeridians(meridians,labels=[True,False,False,True],color='#93994d',size=14)\n",
    "\n",
    "# Station Labels:\n",
    "for stn in ds_stn.station.values:\n",
    "    x, y = ds_stn.sel(station=stn).lon, ds_stn.sel(station=stn).lat\n",
    "    ax.scatter(x,y,zorder=3,c='k')\n",
    "    if stn == '2W6':\n",
    "        ax.text(x,y+0.03,stn,size=14,ha='right',va='bottom')\n",
    "    elif stn == 'NHK':\n",
    "        ax.text(x,y+0.03,stn,size=14,ha='left',va='bottom')\n",
    "    else:\n",
    "        ax.text(x,y+0.03,stn,size=14,ha='center',va='bottom')\n",
    "    if ~np.isnan(ds_stn.sel(station=stn).onshore_min.values.astype(float)):\n",
    "        if stn == 'NAK':\n",
    "            pts = np.arange(50,170.1,5.0)\n",
    "        elif stn == 'NHK':\n",
    "            pts = np.arange(10,130.1,5.0)\n",
    "        elif stn == 'NUI':\n",
    "            pts = np.arange(20,140.1,5.0)\n",
    "        elif stn == 'MTN':\n",
    "            pts = np.arange(80,180.1,5.0)\n",
    "        elif stn == 'APG':\n",
    "            pts = np.arange(60,180.1,5.0)\n",
    "        else:\n",
    "            pts  = np.arange(ds_stn.sel(station=stn).onshore_min.data,ds_stn.sel(station=stn).onshore_max.data,5.0)\n",
    "        npts = pts.size\n",
    "        fill_x = [x]\n",
    "        fill_y = [y]\n",
    "        for dd,wdir in enumerate(pts):\n",
    "            d = 270.0 - wdir    # Convert met degrees to polar\n",
    "            plt_dist = -30000.0 # Met degrees are FROM dir... need negative distance!\n",
    "            fill_x.append(x+plt_dist*np.cos(np.radians(d)))\n",
    "            fill_y.append(y+plt_dist*np.sin(np.radians(d)))\n",
    "\n",
    "        plt.fill(fill_x, fill_y,alpha=0.35,lw=None,color='#e3e022')\n",
    "\n",
    "    ax.tick_params(size=14)\n",
    "    ax.set_title('Station Map',size=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find and validate bay breezes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bb_file_name = 'allDays_TrueSampleRate' # USE THIS ONE FOR FIRST PAPER!\n",
    "#bb_file_name = 'allDays_TrueSampleRate_WRFMinMax' # USE THIS ONE FOR NUMERICAL DETECTION PAPER\n",
    "\n",
    "bay_breeze_ds_f = {}\n",
    "\n",
    "for inland_station_name in inland_station_names[:1]:\n",
    "    for detection_method in detection_methods:\n",
    "        if detection_method == 'StaufferThompson2015':\n",
    "            sample_rate = '60min'\n",
    "        elif detection_method == 'Stauffer2015':\n",
    "            sample_rate = '5min'\n",
    "        elif detection_method == 'Sikora2010':\n",
    "            sample_rate = '60min'            \n",
    "        \n",
    "        new_f_name = '{}{}2019_to_2020_{}_bay_breeze_inland{}_{}.nc'.format(fdir,obs_type,detection_method,inland_station_name,bb_file_name)\n",
    "\n",
    "        if path.exists(new_f_name):\n",
    "            print('loading in full dataset!')\n",
    "            bay_breeze_ds = xr.open_dataset(new_f_name)\n",
    "        else:\n",
    "            #month_count = np.zeros((nstations,period_len))\n",
    "            near_shore_station_num = len(near_shore_stations)\n",
    "            missing_days   = np.zeros((near_shore_station_num,nyears,12)) \n",
    "            analyzed_days  = np.zeros((near_shore_station_num,nyears,12)) \n",
    "            detected_days  = np.zeros((near_shore_station_num,nyears,12)) \n",
    "            validated_days = np.zeros((near_shore_station_num,nyears,12)) \n",
    "\n",
    "            for ss,stn in enumerate(near_shore_stations):   \n",
    "                print('starting station: {}'.format(stn))\n",
    "                ds               = ds_stn.sel(station=stn)\n",
    "                if use_wrf_minmax:\n",
    "                    ds['onshore_min'] = station_dict[stn]['min']\n",
    "                    ds['onshore_max'] = station_dict[stn]['max']\n",
    "                inland_stationsf = ds_stn.sel(station=inland_station_name)\n",
    "                bay_breeze_days = []\n",
    "                if ss == 0:\n",
    "                    bay_breeze_detected    = []\n",
    "                    bay_breeze_validated   = []\n",
    "                    bay_breeze_passage     = []\n",
    "                    bay_breeze_cldORprecip = []\n",
    "                    bay_breeze_day_dict    = {}\n",
    "                    days                   = []\n",
    "\n",
    "                elif ss == 1:\n",
    "                    bay_breeze_detected_f         = np.asarray([[None]*day_count]*near_shore_station_num)\n",
    "                    bay_breeze_detected_f[0,:]    = bay_breeze_detected\n",
    "                    bay_breeze_validated_f        = np.asarray([[None]*day_count]*near_shore_station_num)\n",
    "                    bay_breeze_validated_f[0,:]   = bay_breeze_validated\n",
    "                    bay_breeze_passage_f          = np.asarray([[None]*day_count]*near_shore_station_num)\n",
    "                    bay_breeze_passage_f[0,:]     = bay_breeze_passage\n",
    "                    bay_breeze_cldORprecip_f      = np.asarray([[None]*day_count]*near_shore_station_num)\n",
    "                    bay_breeze_cldORprecip_f[0,:] = bay_breeze_passage\n",
    "\n",
    "                day_count = 0\n",
    "\n",
    "                for yy,year in enumerate(year_range):\n",
    "                    print('Year: {}'.format(year))\n",
    "                    period_start = pd.to_datetime('{0}-{1} 00:00:00'.format(year,date_start))\n",
    "                    period_end   = pd.to_datetime('{0}-{1} 23:59:59'.format(year,date_end))\n",
    "                    month_start  = period_start.month\n",
    "                    period_len   = period_end.month - period_start.month\n",
    "                    for day in pd.date_range(start=period_start, end=period_end):\n",
    "                        station = ds.sel(datetime=slice(str(pd.to_datetime(day)+start_time),\n",
    "                                                    str(pd.to_datetime(day)+end_time)))\n",
    "                        inland_stations = inland_stationsf.sel(datetime=slice(str(pd.to_datetime(day)+start_time),\n",
    "                                                    str(pd.to_datetime(day)+end_time)))\n",
    "\n",
    "                        bay_breeze = DetectBayBreeze(station,inland=inland_stations,verbose=False,resample=True,\n",
    "                                                     sample_rate=sample_rate,show_plot=False,method=detection_method,\n",
    "                                                     remove_cldORprecip=False,light_winds=light_winds)\n",
    "\n",
    "\n",
    "                        month_ind = day.month - month_start            \n",
    "                        if bay_breeze.analyzed:\n",
    "                            analyzed_days[ss,yy,month_ind] += 1\n",
    "                        else:\n",
    "                            missing_days[ss,yy,month_ind] += 1\n",
    "                        if bay_breeze.detected: detected_days[ss,yy,month_ind] += 1\n",
    "                        if bay_breeze.validated: validated_days[ss,yy,month_ind] += 1\n",
    "\n",
    "                        if ss == 0: \n",
    "                            bay_breeze_detected.append(bay_breeze.detected)\n",
    "                            bay_breeze_validated.append(bay_breeze.validated)\n",
    "                            bay_breeze_cldORprecip.append(bay_breeze.cldsORprecip)\n",
    "                        else:\n",
    "                            bay_breeze_detected_f[ss,day_count]    = bay_breeze.detected\n",
    "                            bay_breeze_validated_f[ss,day_count]   = bay_breeze.validated\n",
    "                            bay_breeze_cldORprecip_f[ss,day_count] = bay_breeze.cldsORprecip\n",
    "\n",
    "                        if bay_breeze.detected:\n",
    "                            if bay_breeze.validated:\n",
    "                                fill_color = 'darkgreen'\n",
    "                                bay_breeze_days.append(pd.to_datetime(bay_breeze.passage.values).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                            else:\n",
    "                                fill_color = 'darkred'\n",
    "                            if ss == 0: \n",
    "                                bay_breeze_passage.append(pd.to_datetime(bay_breeze.passage.values).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                            else:\n",
    "                                bay_breeze_passage_f[ss,day_count] = pd.to_datetime(bay_breeze.passage.values).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        else:\n",
    "                            if ss == 0: \n",
    "                                bay_breeze_passage.append('None')\n",
    "                            else:\n",
    "                                bay_breeze_passage_f[ss,day_count] = 'None'\n",
    "\n",
    "                        if ss == 0: days.append(day)\n",
    "                        day_count += 1\n",
    "\n",
    "                bay_breeze_day_dict[stn] = bay_breeze_days   \n",
    "            \n",
    "\n",
    "\n",
    "            bay_breeze_ds = xr.Dataset({\n",
    "                              'analyzed_count': (['station','year','month'],analyzed_days),\n",
    "                               'missing_count': (['station','year','month'],missing_days),\n",
    "                              'detected_count': (['station','year','month'],detected_days),\n",
    "                             'validated_count': (['station','year','month'],validated_days),\n",
    "                               'detected_days': (['station','day'],bay_breeze_detected_f.astype('str')),\n",
    "                              'validated_days': (['station','day'],bay_breeze_validated_f.astype('str')),\n",
    "                            'cldORprecip_days': (['station','day'],bay_breeze_cldORprecip_f.astype('str')),\n",
    "                                'passage_time': (['station','day'],bay_breeze_passage_f.astype('str'))\n",
    "                                        },\n",
    "                             coords={'station': (['station'],near_shore_stations),\n",
    "                                        'year': (['year'],year_range),\n",
    "                                       'month': (['month'],np.arange(month_start,month_start+period_len+1)),\n",
    "                                         'day': (['day'],days)\n",
    "                                     })\n",
    "            bay_breeze_ds.attrs['inland_station'] = inland_station_name\n",
    "\n",
    "            bay_breeze_ds.to_netcdf(new_f_name,'w')\n",
    "        bay_breeze_ds_f[detection_method] = bay_breeze_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_breeze_ds_f = {}\n",
    "for obs_type in ['ASOS','AWOS']:\n",
    "    bay_breeze_ds_f[obs_type] = {}\n",
    "    if obs_type == 'ASOS':\n",
    "        inland_station_names = ['IAD','DCA','MRB','HGR']\n",
    "    elif obs_type == 'AWOS':\n",
    "        inland_station_names = ['IAD','CJR','GAI','FDK']\n",
    "\n",
    "    for detection_method in detection_methods:\n",
    "        bay_breeze_ds_f[obs_type][detection_method] = {}\n",
    "        for inland_station_name in inland_station_names[:1]:\n",
    "\n",
    "            fname = '{}{}2019_to_2020_{}_bay_breeze_inland{}_{}.nc'.format(fdir,obs_type,detection_method,inland_station_name,bb_file_name)\n",
    "\n",
    "            #fname = '{}{}2019_to_2020_{}_bay_breeze_inland{}_allDays.nc'.format(fdir,obs_type,detection_method,inland_station_name)\n",
    "            #fname = '{}{}2019_to_2020_{}_bay_breeze_inland{}_rmCLD.nc'.format(fdir,obs_type,detection_method,inland_station_name)\n",
    "            #fname = '{}{}2019_to_2020_{}_bay_breeze_inland{}_test.nc'.format(fdir,obs_type,detection_method,inland_station_name)\n",
    "            bay_breeze_ds_f[obs_type][detection_method][inland_station_name] = xr.open_dataset(fname)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lessDays = xr.open_dataset('{}{}2019_to_2020_{}_bay_breeze_inland{}_rmCLD.nc'.format(\n",
    "                            fdir,'AWOS','StaufferThompson2015',inland_station_name))\n",
    "\n",
    "moreDays = xr.open_dataset('{}{}2019_to_2020_{}_bay_breeze_inland{}_allDays.nc'.format(\n",
    "                            fdir,'AWOS','StaufferThompson2015',inland_station_name))\n",
    "\n",
    "lowerLightWind = xr.open_dataset('{}{}2019_to_2020_{}_bay_breeze_inland{}_allDays_TrueSampleRate.nc'.format(\n",
    "                            fdir,'AWOS','StaufferThompson2015',inland_station_name))\n",
    "\n",
    "wrf_minmax = xr.open_dataset('{}{}2019_to_2020_{}_bay_breeze_inland{}_allDays_TrueSampleRate_WRFMinMax.nc'.format(\n",
    "                            fdir,'AWOS','StaufferThompson2015',inland_station_name))\n",
    "\n",
    "lessDays.detected_count.sum(dim='station').plot(c='g')\n",
    "moreDays.detected_count.sum(dim='station').plot(c='r')\n",
    "lowerLightWind.detected_count.sum(dim='station').plot(c='b')\n",
    "wrf_minmax.detected_count.sum(dim='station').plot(c='m')\n",
    "plt.show()\n",
    "\n",
    "lessDays.validated_count.sum(dim='station').plot(c='g')\n",
    "moreDays.validated_count.sum(dim='station').plot(c='r')\n",
    "lowerLightWind.validated_count.sum(dim='station').plot(c='b')\n",
    "wrf_minmax.validated_count.sum(dim='station').plot(c='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Sensitivity to Reference Inland Station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = bay_breeze_ds_f['ASOS']['StaufferThompson2015']['IAD'].day.data\n",
    "inland_station_name = 'IAD'\n",
    "\n",
    "bb_passage_dict = {}\n",
    "\n",
    "for detection_method in detection_methods:\n",
    "    bb_passage = []\n",
    "    for obs_type in ['ASOS','AWOS']:\n",
    "        for dd, day in enumerate(days):\n",
    "            day_line = '{}: '.format(str(day)[:10])\n",
    "            if dd == 0:\n",
    "                month_0 = int(str(day)[5:7])\n",
    "            month = int(str(day)[5:7])\n",
    "            if month != month_0:\n",
    "                month_0 = month\n",
    "            \n",
    "            bb_ds = bay_breeze_ds_f[obs_type][detection_method][inland_station_name].sel(day=day)\n",
    "            for ss,stn in enumerate(bb_ds.station.data):\n",
    "                val_bool = bb_ds.sel(station=stn).validated_days.data\n",
    "                if val_bool == 'True':\n",
    "                    bb_passage = np.append(bb_passage,'{} [{}]'.format(bb_ds.sel(station=stn).passage_time.data,stn))\n",
    "    bb_passage = np.unique(bb_passage)\n",
    "    bb_passage_dict[detection_method] = bb_passage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = bay_breeze_ds_f['ASOS']['StaufferThompson2015']['IAD'].day.data\n",
    "inland_station_name = 'IAD'\n",
    "\n",
    "bb_passage_f = []\n",
    "\n",
    "for obs_type in ['ASOS','AWOS']:\n",
    "    for detection_method in detection_methods:\n",
    "        for dd, day in enumerate(days):\n",
    "            day_line = '{}: '.format(str(day)[:10])\n",
    "            if dd == 0:\n",
    "                month_0 = int(str(day)[5:7])\n",
    "            month = int(str(day)[5:7])\n",
    "            if month != month_0:\n",
    "                month_0 = month\n",
    "            \n",
    "            bb_ds = bay_breeze_ds_f[obs_type][detection_method][inland_station_name].sel(day=day)\n",
    "            for ss,stn in enumerate(bb_ds.station.data):\n",
    "                val_bool = bb_ds.sel(station=stn).validated_days.data\n",
    "                if val_bool == 'True':\n",
    "                    bb_passage = np.append(bb_passage,'{} [{}]'.format(bb_ds.sel(station=stn).passage_time.data,stn))\n",
    "bb_passage_f = np.unique(bb_passage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = bay_breeze_ds_f['ASOS']['StaufferThompson2015']['IAD'].day.data\n",
    "\n",
    "num_per_inland_station = {}\n",
    "\n",
    "for obs_type in ['ASOS','AWOS']:\n",
    "    #print(obs_type)\n",
    "    num_per_inland_station[obs_type] = {}\n",
    "    for detection_method in detection_methods:\n",
    "        #print(detection_method)\n",
    "        num_per_inland_station[obs_type][detection_method] = {}\n",
    "        for dd, day in enumerate(days):\n",
    "            day_line = '{}: '.format(str(day)[:10])\n",
    "            if dd == 0:\n",
    "                month_0 = int(str(day)[5:7])\n",
    "            month = int(str(day)[5:7])\n",
    "            if month != month_0:\n",
    "                #print()\n",
    "                month_0 = month\n",
    "            for inland_station_name in list(bay_breeze_ds_f[obs_type][detection_method].keys()):\n",
    "                if dd == 0: num_per_inland_station[obs_type][detection_method][inland_station_name] = 0\n",
    "                got_one_today = False\n",
    "                inland_str = ''\n",
    "                bb_ds = bay_breeze_ds_f[obs_type][detection_method][inland_station_name].sel(day=day)\n",
    "                for ss,stn in enumerate(bb_ds.station.data):\n",
    "                    val_bool = bb_ds.sel(station=stn).validated_days.data\n",
    "                    if val_bool == 'True':\n",
    "                        if got_one_today == True:\n",
    "                            inland_str += ',{}'.format(stn)\n",
    "                        else:\n",
    "                            inland_str = '\\t\\t{}:{}'.format(inland_station_name,stn)\n",
    "                            got_one_today = True\n",
    "                        num_per_inland_station[obs_type][detection_method][inland_station_name] += 1\n",
    "                if inland_str != '': \n",
    "                    day_line += inland_str\n",
    "        #    if day_line != '{}: '.format(str(day)[:10]): print(day_line)\n",
    "        #print('\\n\\n\\n\\n')\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs_type in ['ASOS','AWOS']:\n",
    "    for detection_method in detection_methods:\n",
    "        for inland_station_name in list(bay_breeze_ds_f[obs_type][detection_method].keys()):\n",
    "            print(obs_type,detection_method,inland_station_name,num_per_inland_station[obs_type][detection_method][inland_station_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs_type in ['ASOS','AWOS']:\n",
    "    for detection_method in detection_methods:\n",
    "        for inland_station_name in list(bay_breeze_ds_f[obs_type][detection_method].keys()):\n",
    "            print(obs_type,detection_method,inland_station_name,num_per_inland_station[obs_type][detection_method][inland_station_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs_type in ['ASOS','AWOS']:\n",
    "    for detection_method in detection_methods:\n",
    "        for inland_station_name in list(bay_breeze_ds_f[obs_type][detection_method].keys()):\n",
    "            print(obs_type,detection_method,inland_station_name,num_per_inland_station[obs_type][detection_method][inland_station_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inland_station_name = 'IAD'\n",
    "month_dict = {}\n",
    "\n",
    "header_lines = r'\\begin{table}\\label{LABEL_PLACEHOLDER}' + '\\n' + \\\n",
    "               r'\\bgroup' + '\\n' + r'\\def\\arraystretch{1.5}' + '\\n' + \\\n",
    "               r'\\begin{tabular}{|>{\\centering\\arraybackslash}m{0.07\\textwidth}|' + \\\n",
    "               r'>{\\centering\\arraybackslash}m{0.25\\textwidth}|'*3  + '}' + \\\n",
    "               '\\n' + r'\\hline'\n",
    "footer_lines = r'\\end{tabular}' + '\\n' + r'\\egroup' + '\\n' + r'\\end{table}'\n",
    "\n",
    "detection_method_names_dict = {'StaufferThompson2015':'Stauffer and Thompson (2015)',\n",
    "                                       'Stauffer2015':'Stauffer et al. (2015)',\n",
    "                                         'Sikora2010':'Sikora et al. (2010)'}\n",
    "month_lines_dict = {}\n",
    "for obs_type in ['ASOS','AWOS']:\n",
    "    if obs_type == 'ASOS': inland_station_name = 'IAD'\n",
    "    if obs_type == 'AWOS': inland_station_name = 'IAD'\n",
    "        \n",
    "    #print(header_lines)\n",
    "    #print(obs_type)\n",
    "    methods_line = 'Month & '\n",
    "    month_lines_dict[obs_type] = ['']*12\n",
    "    month_dict[obs_type] = {}\n",
    "    for detection_method in detection_methods:\n",
    "        methods_line += detection_method_names_dict[detection_method] +' & '\n",
    "        #print(detection_method)\n",
    "        month_dict[obs_type][detection_method] = {}\n",
    "        \n",
    "        for dd, day in enumerate(days):\n",
    "            day_line = '{}: '.format(str(day)[:10])\n",
    "            if dd == 0:\n",
    "                month_0 = int(str(day)[5:7])\n",
    "                month_dict[obs_type][detection_method][month_0] = ''\n",
    "            month = int(str(day)[5:7])\n",
    "            if month != month_0:\n",
    "                month_dict[obs_type][detection_method][month_0] = month_dict[obs_type][detection_method][month_0][:-2]\n",
    "                #print(month_0,month_dict[obs_type][detection_method][month_0])\n",
    "                if month_dict[obs_type][detection_method][month_0] == '':\n",
    "                    month_dict[obs_type][detection_method][month_0] = '--'\n",
    "                month_lines_dict[obs_type][month_0-1] += month_dict[obs_type][detection_method][month_0] + ' & '\n",
    "                month_0 = month\n",
    "                month_dict[obs_type][detection_method][month_0] = ''\n",
    "            got_one_today = False\n",
    "            day_stn_list = []\n",
    "            bb_ds = bay_breeze_ds_f[obs_type][detection_method][inland_station_name].sel(day=day)\n",
    "            for ss,stn in enumerate(bb_ds.station.data):\n",
    "                val_bool = bb_ds.sel(station=stn).validated_days.data\n",
    "                if val_bool == 'True':\n",
    "                    if got_one_today == True:\n",
    "                        day_stn_list.append(stn)\n",
    "                    else:\n",
    "                        month_dict[obs_type][detection_method][month_0] += str(int(str(day)[8:10]))\n",
    "                        day_stn_list = [stn]\n",
    "                        got_one_today = True\n",
    "            if len(day_stn_list) > 1:\n",
    "                day_str = '({}), '.format(len(day_stn_list))\n",
    "            else:\n",
    "                day_str = ', '\n",
    "            \n",
    "            if got_one_today:\n",
    "                month_dict[obs_type][detection_method][month_0] += day_str\n",
    "        month_dict[obs_type][detection_method][month_0] = month_dict[obs_type][detection_method][month_0][:-2]\n",
    "        #print(month_0,month_dict[obs_type][detection_method][month_0])\n",
    "        if month_dict[obs_type][detection_method][month_0] == '':\n",
    "            month_dict[obs_type][detection_method][month_0] = '--'\n",
    "        month_lines_dict[obs_type][month_0-1] += month_dict[obs_type][detection_method][month_0] + ' & '\n",
    "        \n",
    "    methods_line = methods_line[:-2]\n",
    "    methods_line += r'\\\\'\n",
    "    print(header_lines.replace('LABEL_PLACEHOLDER','table:{}'.format(obs_type)))\n",
    "    print(methods_line)\n",
    "    print(r'\\hline')\n",
    "    for mm, month in enumerate(month_lines_dict[obs_type]):\n",
    "        month_line = str(mm+1) + ' & ' + month[:-2] + r'\\\\'\n",
    "        print(month_line)\n",
    "        print(r'\\hline')\n",
    "\n",
    "    print(footer_lines)\n",
    "    print('\\n\\n\\n\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(days,asos_days,awos_days,format_text=False,good_days=None):\n",
    "    for dd,day in enumerate(days):\n",
    "        day = day.strip()\n",
    "        #print(day)\n",
    "        if '(' in day:\n",
    "            day = day.split('(')[0]\n",
    "        if day != '--':\n",
    "            days[dd] = int(day)\n",
    "        else:\n",
    "            days[dd] = 0\n",
    "    days = list(np.unique(days))\n",
    "    if 0 in days:\n",
    "        days.remove(0)\n",
    "    \n",
    "    days_temp = days.copy()\n",
    "    if good_days != None:\n",
    "        for dd,day in enumerate(days_temp):\n",
    "            if day not in good_days:\n",
    "                days.remove(day)\n",
    "    if format_text:\n",
    "        formatted_days = days.copy()\n",
    "        for dd,day in enumerate(days):\n",
    "            day = str(day)\n",
    "            if (day in asos_days) & (day not in awos_days):\n",
    "                #print('asos',day)\n",
    "                days[dd] = r'\\textbf{\\textcolor{ASOS}{' + day + '}}'\n",
    "\n",
    "            elif (day in awos_days) & (day not in asos_days):\n",
    "                days[dd] = r'\\textbf{\\textcolor{AWOS}{' + day + '}}'\n",
    "            else:\n",
    "                days[dd] = day\n",
    "\n",
    "        days = ', '.join(np.asarray(days,dtype=str))\n",
    "        if days == '':\n",
    "            days = ' -- '\n",
    "    return(days)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asos_month_lines = month_lines_dict['ASOS']\n",
    "awos_month_lines = month_lines_dict['AWOS']\n",
    "\n",
    "print(header_lines.replace('LABEL_PLACEHOLDER','table:{}'.format('combined')))\n",
    "print(methods_line)\n",
    "print(r'\\hline')\n",
    "for mm,month in enumerate(range(1,13)):\n",
    "    asos_days = asos_month_lines[mm].split('&')\n",
    "    awos_days = awos_month_lines[mm].split('&')\n",
    "\n",
    "    StTh_days = list(np.append(asos_days[0].split(','),awos_days[0].split(',')))\n",
    "    St_days   = list(np.append(asos_days[1].split(','),awos_days[1].split(',')))\n",
    "    Sk_days   = list(np.append(asos_days[2].split(','),awos_days[2].split(',')))\n",
    "    \n",
    "    \n",
    "    StTh_days = remove_duplicates(StTh_days,asos_days[0],awos_days[0],format_text=True)\n",
    "    St_days = remove_duplicates(St_days,asos_days[1],awos_days[1],format_text=True)\n",
    "    Sk_days = remove_duplicates(Sk_days,asos_days[2],awos_days[2],format_text=True)\n",
    "    #if month == 10: \n",
    "    #    print(asos_days[0])\n",
    "    #    print(awos_days[0])\n",
    "    print(str(month) + ' & ' + StTh_days + ' & ' + St_days + ' & ' + Sk_days + r' \\\\')\n",
    "    print(r'\\hline')\n",
    "\n",
    "print(footer_lines)\n",
    "print('\\n\\n\\n\\n')    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Days to check radar/obs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asos_month_lines = month_lines_dict['ASOS']\n",
    "awos_month_lines = month_lines_dict['AWOS']\n",
    "full_month_dict = {}\n",
    "count = 0\n",
    "for mm,month in enumerate(range(1,13)):\n",
    "    asos_days = asos_month_lines[mm].split('&')\n",
    "    awos_days = awos_month_lines[mm].split('&')\n",
    "\n",
    "    StTh_days = list(np.append(asos_days[0].split(','),awos_days[0].split(',')))\n",
    "    St_days   = list(np.append(asos_days[1].split(','),awos_days[1].split(',')))\n",
    "    Sk_days   = list(np.append(asos_days[2].split(','),awos_days[2].split(',')))\n",
    "    \n",
    "    \n",
    "    StTh_days = remove_duplicates(StTh_days,asos_days[0],awos_days[0])\n",
    "    St_days = remove_duplicates(St_days,asos_days[1],awos_days[1])\n",
    "    Sk_days = remove_duplicates(Sk_days,asos_days[2],awos_days[2])\n",
    "    full_day_list = list(np.unique(np.asarray(np.concatenate([StTh_days,St_days,Sk_days]),dtype=int)))\n",
    "    for dd,day in enumerate(full_day_list):\n",
    "        full_day_list[dd] = '2019-{0:02d}-{1:02d}'.format(month,day)\n",
    "        \n",
    "    full_month_dict[month] = full_day_list\n",
    "    for day in full_day_list:\n",
    "        for day_time in bb_passage_f:\n",
    "            date = day_time[:10]\n",
    "            if day == date:\n",
    "                print(\"'{}': ' ',\".format(day_time))\n",
    "        count += 1\n",
    "    print()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "command_str = 'curl -O '\n",
    "gif_loc = 'https://www.wpc.ncep.noaa.gov/archives/sfc/{0:04d}/namussfc{0:04d}{1:02d}{2:02d}{3:02d}.gif'\n",
    "\n",
    "for mm,month in enumerate(full_month_dict):\n",
    "    for day in full_month_dict[month]:\n",
    "        day = pd.to_datetime(day)\n",
    "        f_loc = gif_loc.format(day.year,day.month,day.day,18)\n",
    "        print(command_str + f_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_days import analysis_days_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_analysis_dict = analysis_days_dict[bb_file_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "good_days = []\n",
    "for day in day_analysis_dict:\n",
    "    day_analysis = day_analysis_dict[day].strip()\n",
    "    if (day_analysis == 'radar clear. no synoptic.') or (day_analysis == 'radar clear. SWT in area.'):\n",
    "    #if (day_analysis == 'radar clear. no synoptic.'):\n",
    "        count+=1\n",
    "        good_days = np.append(good_days,day[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asos_month_lines = month_lines_dict['ASOS']\n",
    "awos_month_lines = month_lines_dict['AWOS']\n",
    "\n",
    "print(header_lines.replace('LABEL_PLACEHOLDER','table:{}'.format('combined')))\n",
    "print(methods_line)\n",
    "print(r'\\hline')\n",
    "for mm,month in enumerate(range(1,13)):\n",
    "    asos_days = asos_month_lines[mm].split('&')\n",
    "    awos_days = awos_month_lines[mm].split('&')\n",
    "\n",
    "    StTh_days = list(np.append(asos_days[0].split(','),awos_days[0].split(',')))\n",
    "    St_days   = list(np.append(asos_days[1].split(','),awos_days[1].split(',')))\n",
    "    Sk_days   = list(np.append(asos_days[2].split(','),awos_days[2].split(',')))\n",
    "    good_days_month = []\n",
    "    for day in good_days:\n",
    "        if day[5:7] == '{0:02d}'.format(month):\n",
    "            good_days_month.append(int(day[8:10]))\n",
    "    good_days_month = list(np.unique(good_days_month))\n",
    "#    print(good_days_month)\n",
    "    StTh_days = remove_duplicates(StTh_days,asos_days[0],awos_days[0],format_text=True,good_days=good_days_month)\n",
    "    St_days   = remove_duplicates(St_days,  asos_days[1],awos_days[1],format_text=True,good_days=good_days_month)\n",
    "    #Sk_days   = remove_duplicates(Sk_days,  asos_days[2],awos_days[2],format_text=True,good_days=good_days_month)\n",
    "    Sk_days   = remove_duplicates(Sk_days,  asos_days[2],awos_days[2],format_text=True,good_days=None)\n",
    "    #if month == 10: \n",
    "    #    print(asos_days[0])\n",
    "    #    print(awos_days[0])\n",
    "    print(str(month) + ' & ' + StTh_days + ' & ' + St_days + ' & ' + Sk_days + r' \\\\')\n",
    "    print(r'\\hline')\n",
    "\n",
    "print(footer_lines)\n",
    "print('\\n\\n\\n\\n')    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# July BB Days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVED CLOUDY / RAINY DAYS AUTOMATICALLY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = 7\n",
    "mm = month-1\n",
    "\n",
    "asos_days = asos_month_lines[mm].split('&')\n",
    "awos_days = awos_month_lines[mm].split('&')\n",
    "\n",
    "StTh_days = list(np.append(asos_days[0].split(','),awos_days[0].split(',')))\n",
    "St_days   = list(np.append(asos_days[1].split(','),awos_days[1].split(',')))\n",
    "Sk_days   = list(np.append(asos_days[2].split(','),awos_days[2].split(',')))\n",
    "\n",
    "good_days_month = []\n",
    "for day in good_days:\n",
    "    if day[5:7] == '{0:02d}'.format(month):\n",
    "        good_days_month.append(int(day[8:10]))\n",
    "good_days_month = list(np.unique(good_days_month))\n",
    "\n",
    "StTh_days = remove_duplicates(StTh_days,asos_days[0],awos_days[0],format_text=False,good_days=good_days_month)\n",
    "St_days   = remove_duplicates(St_days,  asos_days[1],awos_days[1],format_text=False,good_days=good_days_month)\n",
    "Sk_days   = remove_duplicates(Sk_days,  asos_days[2],awos_days[2],format_text=False,good_days=None)\n",
    "july_days_dict = {'StaufferThompson2015':StTh_days,\n",
    "                  'Stauffer2015':St_days,\n",
    "                  'Sikora2010':Sk_days                  \n",
    "                 }\n",
    "print(\"bb_observed_dates = {\")\n",
    "for detection_method in july_days_dict.keys():\n",
    "    print(\"                     {}\".format(\"'{}'\".format(detection_method)).rjust(22) + \" : {\")\n",
    "    days = july_days_dict[detection_method]\n",
    "    for day in days:\n",
    "        day_str = '2019-{0:02d}-{1:02d}'.format(month,day)\n",
    "        day_stn = []\n",
    "        for bb_day in bb_passage_dict[detection_method]:\n",
    "            if day_str in bb_day:\n",
    "                day_stn.append(bb_day.split(' ')[-1].replace('[','').replace(']',''))\n",
    "        print(\"                                               '{}' : {},\".format(day_str,day_stn))\n",
    "        #print(day_str)\n",
    "    print('                                              },')\n",
    "print(\"                     }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO REMOVED DAYS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = 7\n",
    "mm = month-1\n",
    "\n",
    "asos_days = asos_month_lines[mm].split('&')\n",
    "awos_days = awos_month_lines[mm].split('&')\n",
    "\n",
    "StTh_days = list(np.append(asos_days[0].split(','),awos_days[0].split(',')))\n",
    "St_days   = list(np.append(asos_days[1].split(','),awos_days[1].split(',')))\n",
    "Sk_days   = list(np.append(asos_days[2].split(','),awos_days[2].split(',')))\n",
    "\n",
    "good_days_month = []\n",
    "for day in good_days:\n",
    "    if day[5:7] == '{0:02d}'.format(month):\n",
    "        good_days_month.append(int(day[8:10]))\n",
    "good_days_month = list(np.unique(good_days_month))\n",
    "\n",
    "StTh_days = remove_duplicates(StTh_days,asos_days[0],awos_days[0],format_text=False,good_days=good_days_month)\n",
    "St_days   = remove_duplicates(St_days,  asos_days[1],awos_days[1],format_text=False,good_days=good_days_month)\n",
    "Sk_days   = remove_duplicates(Sk_days,  asos_days[2],awos_days[2],format_text=False,good_days=good_days_month)\n",
    "july_days_dict = {'StaufferThompson2015':StTh_days,\n",
    "                  'Stauffer2015':St_days,\n",
    "                  'Sikora2010':Sk_days                  \n",
    "                 }\n",
    "print(\"bb_observed_dates = {\")\n",
    "for detection_method in july_days_dict.keys():\n",
    "    print(\"                     {}\".format(\"'{}'\".format(detection_method)).rjust(22) + \" : {\")\n",
    "    days = july_days_dict[detection_method]\n",
    "    for day in days:\n",
    "        day_str = '2019-{0:02d}-{1:02d}'.format(month,day)\n",
    "        day_stn = []\n",
    "        for bb_day in bb_passage:\n",
    "            if day_str in bb_day:\n",
    "                day_stn.append(bb_day.split(' ')[-1].replace('[','').replace(']',''))\n",
    "        print(\"                                               '{}' : {},\".format(day_str,day_stn))\n",
    "        #print(day_str)\n",
    "    print('                                              },')\n",
    "print(\"                     }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmc",
   "language": "python",
   "name": "mmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
